{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"FinalData/channel_first_data.npz\")\n",
    "normalData = data[\"input\"]\n",
    "shuffledData = data[\"input\"]\n",
    "np.random.shuffle(shuffledData)\n",
    "\n",
    "trainInput,trainOutput = shuffledData[0:15000],shuffledData[0:15000]\n",
    "testInput,testOutput = shuffledData[15000:],shuffledData[15000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import Sokoban\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "dataTensor = tf.convert_to_tensor(normalData, dtype=tf.float32)\n",
    "\n",
    "def fulldata_loss(y_true, y_pred):\n",
    "    loss = categorical_crossentropy(y_true, y_pred)\n",
    "    prob = 1 - K.mean(dataTensor, axis=[0,1,2], keepdims=True)\n",
    "    prob = tf.divide(prob, tf.keras.backend.max(prob))\n",
    "    prob = K.sum(tf.multiply(y_true, K.reshape(prob,(1,1,1,7))), axis=3)\n",
    "    \n",
    "    return K.mean(tf.multiply(loss, prob), axis=[1,2])\n",
    "\n",
    "def minibatch_loss(y_true, y_pred):\n",
    "    loss = categorical_crossentropy(y_true, y_pred)\n",
    "    prob = 1 - K.mean(y_true, axis=[0,1,2], keepdims=True)\n",
    "    prob = tf.divide(prob, tf.keras.backend.max(prob))\n",
    "    prob = K.sum(tf.multiply(y_true, K.reshape(prob,(1,1,1,7))), axis=3)\n",
    "    \n",
    "    return K.mean(tf.multiply(loss, prob), axis=[1,2])\n",
    "    \n",
    "def sample_loss(y_true, y_pred):\n",
    "    loss = categorical_crossentropy(y_true, y_pred)\n",
    "    prob = 1 - K.mean(y_true, axis=[1,2], keepdims=True)\n",
    "    prob = tf.divide(prob, tf.keras.backend.max(prob))\n",
    "    prob = K.sum(tf.multiply(K.reshape(K.repeat_elements(prob, 100, axis=1),(-1,10,10,7)), y_true),axis=3)\n",
    "\n",
    "    return K.mean(tf.multiply(loss, prob), axis=[1,2])\n",
    "    \n",
    "def normal_loss(y_true, y_pred):\n",
    "    return K.mean(categorical_crossentropy(y_true, y_pred), axis=[1,2])\n",
    "\n",
    "def get_contractive_loss(loss_fn, lam=1e-4):\n",
    "    def contractive_loss(y_true, y_pred):\n",
    "        cce = loss_fn(y_true, y_pred)\n",
    "        W = K.variable(value=autoencoder.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
    "        W = K.transpose(W)  # N_hidden x N\n",
    "        h = autoencoder.get_layer('encoded').output\n",
    "        h = K.reshape(h,(-1,encoding))\n",
    "        dh = h * (1 - h)  # N_batch x N_hidden\n",
    "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
    "        contractive = K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
    "        return cce + lam * contractive\n",
    "    \n",
    "    return contractive_loss\n",
    "\n",
    "def numpyToString(array):\n",
    "    gameCharacters=\"# @$.+*\"\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(gameCharacters))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(gameCharacters))\n",
    "    intArray = np.argmax(array, axis=2)\n",
    "    output=\"\"\n",
    "    for (i,j), index in np.ndenumerate(intArray):\n",
    "        if i > 0 and j == 0:\n",
    "            output += \"\\n\"\n",
    "        output += int_to_char[index]\n",
    "    return output\n",
    "\n",
    "def convertStringTo2DArray(string):\n",
    "    return np.array([list(l) for l in string.split(\"\\n\")])\n",
    "\n",
    "def getOneHotEncodingMap(data):\n",
    "    gameCharacters=\"# @$.+*\"\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(gameCharacters))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(gameCharacters))\n",
    "\n",
    "    encodingData = np.zeros((data.shape[0], data.shape[1], len(gameCharacters)))\n",
    "    for (i,j),c in np.ndenumerate(data):\n",
    "        index=char_to_int[c]\n",
    "        encodingData[i][j][index]=1\n",
    "    return encodingData\n",
    "\n",
    "def getHammingDistance(data1, data2):\n",
    "    data1=np.argmax(data1, axis=3)\n",
    "    data2=np.argmax(data2, axis=3)\n",
    "    result=np.absolute(data1-data2)\n",
    "    result=np.clip(result,0,1)\n",
    "    return np.sum(result)/data1.shape[0]\n",
    "\n",
    "def combineTwoStrings(str1, str2):\n",
    "    if len(str1) == 0:\n",
    "        return str2\n",
    "    if len(str2) == 0:\n",
    "        return str1\n",
    "    lines1=str1.split(\"\\n\")\n",
    "    lines2=str2.split(\"\\n\")\n",
    "    linesOut=[]\n",
    "    for i in range(10):\n",
    "        linesOut.append(lines1[i] + \"  \" + lines2[i])\n",
    "    return \"\\n\".join(linesOut)\n",
    "\n",
    "def getNoisedState(numpyLevel, randomSize=20):\n",
    "    stringLvl=numpyToString(numpyLevel)\n",
    "    state=Sokoban.State()\n",
    "    state.stringInitialize(stringLvl.split(\"\\n\"))\n",
    "    for i in range(randomSize):\n",
    "        if random.random() < 0.5:\n",
    "            state.update(0, 2*random.randint(0,1) - 1)\n",
    "        else:\n",
    "            state.update(2*random.randint(0,1) - 1, 0)\n",
    "    return getOneHotEncodingMap(convertStringTo2DArray(str(state)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "\n",
    "encoding=8\n",
    "\n",
    "input_level=layers.Input(shape=(10,10,7))\n",
    "encoded_level=layers.Input(shape=(encoding,))\n",
    "\n",
    "x = layers.ZeroPadding2D(padding=(1,1))(input_level)\n",
    "x = layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2,2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2,2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((3,3), padding=\"same\")(x)\n",
    "x = layers.Dense(encoding, activation=\"relu\")(x)\n",
    "encoded = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Reshape((1,1,encoding))(encoded)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D((3,3))(x)\n",
    "x = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2D(7, (3,3), padding=\"valid\")(x)\n",
    "decoded = layers.Softmax(axis=3)(x)\n",
    "\n",
    "autoencoder = Model(input_level, decoded)\n",
    "encoder = Model(input_level, encoded)\n",
    "decoded = autoencoder.layers[-9](encoded_level)\n",
    "decoded = autoencoder.layers[-8](decoded)\n",
    "decoded = autoencoder.layers[-7](decoded)\n",
    "decoded = autoencoder.layers[-6](decoded)\n",
    "decoded = autoencoder.layers[-5](decoded)\n",
    "decoded = autoencoder.layers[-4](decoded)\n",
    "decoded = autoencoder.layers[-3](decoded)\n",
    "decoded = autoencoder.layers[-2](decoded)\n",
    "decoded = autoencoder.layers[-1](decoded)\n",
    "decoder = Model(encoded_level,decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=normal_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files=os.listdir(\"Weights/\")\n",
    "for f in files:\n",
    "    if not os.path.isdir(\"Weights/\" + f):\n",
    "        os.remove(\"Weights/\" + f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "autoencoder.fit(trainInput, trainOutput\n",
    "                ,epochs=2000\n",
    "                ,batch_size=128\n",
    "                ,shuffle=True\n",
    "                ,validation_data=(testInput, testOutput)\n",
    "                ,callbacks=[ModelCheckpoint(\"Weights/model_{epoch:02d}_{loss:.2f}_{val_loss:.2f}.hdf5\", \n",
    "                                            save_weights_only=False, save_best_only=False, period=20)]\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "usedInput = trainInput\n",
    "usedOutput = trainInput\n",
    "\n",
    "autoencoder.load_weights(\"Weights/model_2000_0.17_0.33.hdf5\")\n",
    "\n",
    "encodedLevels = encoder.predict(usedInput)\n",
    "decodedLevels = decoder.predict(encodedLevels)\n",
    "print(autoencoder.evaluate(usedInput, usedOutput, batch_size=128))\n",
    "\n",
    "stringNormalLevels = \"\"\n",
    "stringDecodedLevels = \"\"\n",
    "for i in range(9):\n",
    "    levelNumber=random.randint(0,len(usedInput)-1)\n",
    "    stringNormalLevels = combineTwoStrings(stringNormalLevels, numpyToString(usedInput[levelNumber]))\n",
    "    stringDecodedLevels = combineTwoStrings(stringDecodedLevels, numpyToString(decodedLevels[levelNumber]))\n",
    "    \n",
    "print(stringNormalLevels)\n",
    "print()\n",
    "print(stringDecodedLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a certain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files=os.listdir(\"FinalWeights/Conv_16/\")\n",
    "for f in files:\n",
    "    if \"hdf5\" not in f or \"32\" in f:\n",
    "        continue\n",
    "    autoencoder.load_weights(\"FinalWeights/Conv_16/\" + f)\n",
    "    encodedLevels = encoder.predict(normalData)\n",
    "    decodedLevels = decoder.predict(encodedLevels)\n",
    "    print(f)\n",
    "    print(getHammingDistance(normalData,decodedLevels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpyToString(normalData[0]))\n",
    "print()\n",
    "print(numpyToString(getNoisedState(normalData[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
